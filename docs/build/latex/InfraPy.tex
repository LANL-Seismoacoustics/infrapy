%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}


\title{InfraPy Documentation}
\date{Jun 05, 2020}
\release{1.0}
\author{F.Dannemann Dugick, O.Marcillo, P.Blom, J.Webster, }
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Contents}
\label{\detokenize{index:contents}}

\section{Authorship}
\label{\detokenize{authorship:authorship}}\label{\detokenize{authorship::doc}}
Infrapy was built upon previous similar (InfraMonitor) tools and developed by the LANL SeismoAcoustic Team.

Omar Marcillo
omarcillo at lanl.gov

Philip Blom
pblom at lanl.gov

Jeremy Webster
jwebster at lanl.gov

Fransiska Dannemann Dugick
fransiska at lanl.gov


\section{Installation}
\label{\detokenize{installation:installation}}\label{\detokenize{installation:id1}}\label{\detokenize{installation::doc}}

\subsection{Operating Systems}
\label{\detokenize{installation:operating-systems}}
Infrapy can currently be installed on machines running newer versions of Linux or Apple OSX.  A Windows-compatible version is in development.


\subsection{Anaconda}
\label{\detokenize{installation:anaconda}}
The installation of infrapy currently depends on Anaconda to resolve and download the correct python libraries. So if you don’t currently have anaconda installed
on your system, please do that first.

Anaconda can be downloaded from \sphinxurl{https://www.anaconda.com/distribution/}. Either 3.x or 2.x will work since the numbers refer to the Python version of the default
environment.  Infrapy’s installation will create a new environment and will install the version of Python that it needs into that environment.


\subsection{Infrapy Installation}
\label{\detokenize{installation:infrapy-installation}}
Once Anaconda is installed, you can install infrapy by navigating to the base directory of the infrapy package (there will be a file there
named infrapy\_env.yml), and run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYGZgt{} conda env create \PYGZhy{}f infrapy\PYGZus{}env.yml
\end{sphinxVerbatim}

If this command executes correctly and finishes without errors, it should print out instructions on how to activate and deactivate the new environment:

To activate the environment, use:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYGZgt{} conda activate infrapy\PYGZus{}env
\end{sphinxVerbatim}

To deactivate an active environment, use

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYGZgt{} conda deactivate
\end{sphinxVerbatim}


\subsection{Testing}
\label{\detokenize{installation:testing}}
Once the installation is complete, you can test some things by first activating the environment with:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYGZgt{} conda activate infrapy\PYGZus{}env
\end{sphinxVerbatim}

Then navigate to the /example directory located in the infrapy base directory, and run the test scripts via something like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYGZgt{} python test\PYGZus{}beamforming.py
\end{sphinxVerbatim}

If infrapy was successfully installed, all of the test scripts should run and finish without any errors.


\subsection{Running the InfraView GUI Application}
\label{\detokenize{installation:running-the-infraview-gui-application}}
Once installation is complete, and the new environment is activated, you can run the GUI with the command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYGZgt{} infraview
\end{sphinxVerbatim}


\section{Quickstart}
\label{\detokenize{quickstart:quickstart}}\label{\detokenize{quickstart:id1}}\label{\detokenize{quickstart::doc}}
A series of scripts illustrating how to use infrapy subroutines as stand-alone modules are found in the /examples folder.
The jupyter notebook documenting these steps is found in /tutorials/Quick Start.ipynb.  The notebook can be run by installing jupyter notebook via conda.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{conda} \PYG{n}{install} \PYG{n}{jupyter} \PYG{n}{notebook}
\end{sphinxVerbatim}

Beamforming:
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Run Bartlett, Capon or Generalized Least Squares beamforming processes on an hour-long dataset from the BRP array in Utah

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{python} \PYG{n}{test\PYGZus{}beamforming}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{1}
\item {} 
Visualize beamforming results in the sx/sy space

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{python} \PYG{n}{test\PYGZus{}slowness}\PYG{o}{\PYGZhy{}}\PYG{n}{grid}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

Detection:
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Run detection on the series of beamforming results produced in the above step

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{python} \PYG{n}{test\PYGZus{}detection}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

Association
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Associate a number of detections contained in a .dat file (/data/detection\_set1.dat or /data/detection\_set2.dat)

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{python} \PYG{n}{test\PYGZus{}assoc}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}

Location
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Test the Bayesian Infrasonic Source Localization (BISL) methodology using a set of provided detections (/data/detection\_set1.dat or /data/detection\_set2.dat).  Location will be run twice, once assuming uniform atmospheric propagation and a second time applying provided atmospheric propagation priors for the Western US (see Blom et al., 2015 for further explanation)

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{python} \PYG{n}{test\PYGZus{}bisl}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\section{Interfacing with Pisces}
\label{\detokenize{pisces:interfacing-with-pisces}}\label{\detokenize{pisces:pisces}}\label{\detokenize{pisces::doc}}
Infrapy leverages pisces to connect with and process data in either local sqlite databases or oracle databases. More information about pisces can be found at \sphinxurl{https://jkmacc-lanl.github.io/pisces/}.


\subsection{Converting Data into Sqlite Databases}
\label{\detokenize{pisces:converting-data-into-sqlite-databases}}
Data in miniseed or sac formats can be loaded into a sqlite database for pipeline processing using commands from pisces.
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
mseed to database (ms2db.py)

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{ms2db}\PYG{o}{.}\PYG{n}{py} \PYG{n}{sqlite}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{o}{/}\PYG{n}{example}\PYG{o}{.}\PYG{n}{sqlite} \PYG{n}{mslist}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{1}
\item {} 
sac to database (sac2db)

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{pisces} \PYG{n}{sac2db} \PYG{n}{sqlite}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{o}{/}\PYG{n}{example}\PYG{o}{.}\PYG{n}{sqlite} \PYG{o}{*}\PYG{o}{.}\PYG{n}{sac}
\end{sphinxVerbatim}

As infrapy is an array processing tool, after your sqlite database is created, you will need to update the REFSTA for each array using update\_refsta.py

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{update\PYGZus{}refsta}\PYG{o}{.}\PYG{n}{py} \PYG{n}{sqlite}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{o}{/}\PYG{n}{example}\PYG{o}{.}\PYG{n}{sqlite} \PYG{o}{\PYGZlt{}}\PYG{n}{array} \PYG{n}{name}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}

You can update the calibration for each array using update\_calib.py

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{update\PYGZus{}calib}\PYG{o}{.}\PYG{n}{py} \PYG{n}{sqlite}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{o}{/}\PYG{n}{example}\PYG{o}{.}\PYG{n}{sqlite} \PYG{o}{\PYGZlt{}}\PYG{n}{array} \PYG{n}{name}\PYG{o}{\PYGZgt{}} \PYG{o}{\PYGZlt{}}\PYG{n}{calibration}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}


\subsection{Connecting to a SQL Database}
\label{\detokenize{pisces:connecting-to-a-sql-database}}
Infrapy employs two main methods for connecting to either Oracle or sqlite databases.  Example files to facilitate these connections are found in tutorials/.


\subsubsection{Defining Schema Specific Tables}
\label{\detokenize{pisces:defining-schema-specific-tables}}
Pipeline processing in infrapy utilizes information from CSS3.0 Site and Wfdisc tables.  If your database schema differs from the CSS3.0 schema in any way, you can define the differences using a \_global.py file.  An example \_global.py file is found in tutorial/ .


\subsubsection{Connection within pipeline processing configuration file}
\label{\detokenize{pisces:connection-within-pipeline-processing-configuration-file}}
The first three lines of your configuration file define the database you will connect to:

\sphinxstylestrong{Example Configuration File for Sqlite Processing (Sqlite\_Config.txt)}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[database] \PYGZsh{} required
\PYGZsh{} url to database where you have the pointers to data and metadata
url = sqlite:///example.sqlite
\PYGZsh{} schema specific tables for your site and wfdisc files.  If you are processing in a sqlite database, these variables will refer to schema specified in pisces. If you are processing in an oracle database, these variables will refer to schema specified in your global\PYGZus{}.py file
site = pisces.tables.css3:Site
wfdisc = pisces.tables.css3:Wfdisc
\end{sphinxVerbatim}

\sphinxstylestrong{Example Configuration File for Oracle DB Processing (Oracle\_Config.txt)}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[database] \PYGZsh{} required
url = oracle://\PYGZlt{}database name\PYGZgt{}:\PYGZlt{}port\PYGZgt{}
site = global\PYGZus{}:Site
wfdisc = global\PYGZus{}:Wfdisc\PYGZus{}raw
\end{sphinxVerbatim}


\subsubsection{Connection with a db.cfg file}
\label{\detokenize{pisces:connection-with-a-db-cfg-file}}
Some modules in infrapy (db2sac) require a .cfg file to establish connection with a database.  Examples are found in tutorial/ . More information can be found in the pisces documentation.

\sphinxstylestrong{Example Configuration File for Oracle DB Processing (oracle\_connection.cfg)}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[database] \PYGZsh{} required
url = oracle://\PYGZlt{}db name\PYGZgt{}:\PYGZlt{}db port\PYGZgt{}
site = global\PYGZus{}:Site
wfdisc = global\PYGZus{}:Wfdisc\PYGZus{}raw
origin = global\PYGZus{}:Origin
\end{sphinxVerbatim}

\sphinxstylestrong{Example Configuration File for Sqlite Processing (sqlite\_connection.cfg)}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[database] \PYGZsh{} required
url = sqlite:///example.sqlite
site = pisces.tables.css3:Site
wfdisc = pisces.tables.css3:Wfdisc
origin = pisces.tables.css3:Origin
\end{sphinxVerbatim}


\section{Algorithms}
\label{\detokenize{algorithms:algorithms}}\label{\detokenize{algorithms:id1}}\label{\detokenize{algorithms::doc}}\begin{itemize}
\item {} 
Analyst methods are modular so that results from other processing tools can be used in later analysis steps.

\item {} 
Algorithms are written to be data agnostic so analysis can be performed regardless of data source once IO method is established

\end{itemize}


\subsection{Station Level Processing}
\label{\detokenize{algorithms:station-level-processing}}

\subsubsection{Array Processing}
\label{\detokenize{algorithms:beamforming}}\begin{itemize}
\item {} 
Beamforming estimates parameters of coherent signals

\item {} 
Capabilities include methods to characterize transients as well as persistent signals

\item {} 
Transient signals are identified using standard Bartlett beaming

\item {} 
Persistent signals can be investigated using Minimum Variance Distortionless Response (MVDR) or MUltiple Signal Classification (MUSIC) algorithms

\end{itemize}


\subsubsection{The Adaptive F-Detector}
\label{\detokenize{algorithms:afd}}\begin{itemize}
\item {} 
Adaptive Fisher statistics determine when to declare a detection

\end{itemize}


\subsection{Network Level Processing}
\label{\detokenize{algorithms:network-level-processing}}

\subsubsection{Association}
\label{\detokenize{algorithms:association}}\begin{itemize}
\item {} 
Events are identified using a pair-based Bayesian algorithm that defines the association between detection pairs from their joint-likelihood and identifies events via hierarchical clustering analysis

\item {} 
Current implementation utilizes only spatial and temporal coincidence, but additional detection information can further improve event identification

\item {} 
Evaluation using a synthetic data set shows some mixing of spatially similar events poorly resolved by network geometry and occasional inclusion of “noise” detections in event clustering

\end{itemize}


\subsubsection{Bayesian Infrasonic Source Localization}
\label{\detokenize{algorithms:localization}}\begin{itemize}
\item {} 
Event analysis using the Bayesian Infrasonic Source Localization (BISL) methodology to estimate both the spatial location of the event as well as the origin time with quantified uncertainty

\item {} 
Preliminary analysis of the back projections can be used to define the spatial region of interest or it can be specified by the analysis

\item {} 
Analysis identifies the maximum posteriori solution

\item {} 
The marginalized spatial distribution is approximated as 2d-normal to define 95 and 99\% confidence ellipse bounds

\item {} 
The marginalized temporal distribution is analyzed to identify the exact 95 and 99\% confidence bounds

\item {} 
Likelihood definitions relating detection parameters to spatial and temporal source characteristics are shared between association and localization analysis for consistency
\begin{quote}


\paragraph{Array Processing}
\label{\detokenize{beamforming:array-processing}}\label{\detokenize{beamforming:beamforming}}\label{\detokenize{beamforming::doc}}
The use of infrasonic arrays, specifically for CTBT applications, is preferable due to the inherent reduction in signal-to-noise ratios (SNR) originating from the summation of four or more recordings at each array. The nature of a decision-rule based detector requires data that has been pre-processed using beamforming methods. Beamforming, a form of array processing, is the first step in the analysis of data from infrasonic arrays.    Conventional beamforming methods (Bartlett, Capon) separate coherent and incoherent parts of a signal through the assumption of planar waves arriving at the array.  A signal backazimuth and slowness can be estimated as signals are shifted to account for travel time differentials across array elements, bringing the signal into phase across as the noise deconstructively cancels out.  In the classical, or Bartlett methodology , data records on each array element are time-shifted versions of the other with local noise,

See the following for more references on beamforming:
\sphinxhref{https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2000RG000100}{Rost and Thomas 2002}
\sphinxhref{https://link.springer.com/chapter/10.1007/978-0-387-30441-0\_81}{Olson and Szuberla 2010}
\sphinxhref{https://asa.scitation.org/doi/full/10.1121/1.4818940}{Costley 2013}


\paragraph{The Adaptive F-Detector}
\label{\detokenize{detection:the-adaptive-f-detector}}\label{\detokenize{detection:afd}}\label{\detokenize{detection::doc}}
The standard F-detector is based on decision rules for the F-statistic, which provides an estimate of the signal’s beam power and is calculated as the power in the beam divided by the average over all channels of the power difference between the beam and the individual channels cite\{Blandford:1982\}.  The AFD accounts for both correlated and uncorrelated noise through an increase in the value of the F-statistic required to declare a detection based upon background F-values being elevated from coherent or persistent noise sources. The figure below illustrates how the AFD remaps the F-statistic through the application of a C-value, which effective reduces the detection threshold (p-value) and decreases the number of noise-related detections.

\noindent\sphinxincludegraphics{{AFD}.png}

See the following for more references on the Adaptive F-Detector:

\sphinxhref{https://doi.org/10.1111/j.1365-246X.2008.03912.x}{Arrowmsith et al., 2008}
\sphinxhref{https://pubs.geoscienceworld.org/ssa/bssa/article/99/1/449/342096}{Arrowsmith et al., 2009}


\paragraph{Association}
\label{\detokenize{association:association}}\label{\detokenize{association:id1}}\label{\detokenize{association::doc}}\begin{itemize}
\item {} 
Events are identified using a pair-based Bayesian algorithm that defines the association between detection pairs from their joint-likelihood and identifies events via hierarchical clustering analysis

\item {} 
Current implementation utilizes only spatial and temporal coincidence, but additional detection information can further improve event identification

\item {} 
Evaluation using a synthetic data set shows some mixing of spatially similar events poorly resolved by network geometry and occasional inclusion of “noise” detections in event clustering

\end{itemize}

See the following for more references on Association:
\sphinxhref{https://academic.oup.com/gji/advance-article-abstract/doi/10.1093/gji/ggaa105/5800992}{Blom et al., 2020}


\paragraph{Bayesian Infrasonic Source Localization}
\label{\detokenize{localization:bayesian-infrasonic-source-localization}}\label{\detokenize{localization:localization}}\label{\detokenize{localization::doc}}\begin{itemize}
\item {} 
Event analysis uses the Bayesian Infrasonic Source Localization (BISL) methodology to estimate both the spatial location of the event as well as the origin time with quantified uncertainty

\item {} 
Preliminary analysis of the back projections can be used to define the spatial region of interest or it can be specified by the analysis

\item {} 
Analysis identifies the maximum posteriori solution

\item {} 
The marginalized spatial distribution is approximated as 2d-normal to define 95 and 99\% confidence ellipse bounds

\item {} 
The marginalized temporal distribution is analyzed to identify the exact 95 and 99\% confidence bounds

\item {} 
Likelihood definitions relating detection parameters to spatial and temporal source characteristics are shared between association and localization analysis for consistency

\end{itemize}

See the following for more references on BISL:
\sphinxhref{https://academic.oup.com/gji/article/181/1/399/718964}{Modrak et al., 2010}
\sphinxhref{https://academic.oup.com/gji/article/196/1/375/586767}{Marcillo et al., 2013}
\sphinxhref{https://academic.oup.com/gji/article/203/3/1682/2594791}{Blom et al., 2015}
\end{quote}

\end{itemize}


\section{Data Processing Flow}
\label{\detokenize{processing_model:data-processing-flow}}\label{\detokenize{processing_model:dataprocess}}\label{\detokenize{processing_model::doc}}
Data can be processed using InfraPy in a variety of ways.  See the images below for examples of data processing workflows.


\subsection{The Adaptive F-Detector}
\label{\detokenize{processing_model:afd}}
\noindent\sphinxincludegraphics{{detection_data_flow}.png}


\subsection{Association}
\label{\detokenize{processing_model:association}}
\noindent\sphinxincludegraphics{{association_data_flow}.png}


\subsection{Bayesian Infrasonic Source Localization}
\label{\detokenize{processing_model:localization}}
\noindent\sphinxincludegraphics{{localization_data_flow}.png}


\subsubsection{Stand-Alone Processing}
\label{\detokenize{standalone:stand-alone-processing}}\label{\detokenize{standalone:standalone}}\label{\detokenize{standalone::doc}}
Modules within Infrapy can be run ‘stand-alone’ as package imports following the scripts found in the /examples folder


\subsubsection{Running Pipeline Processing in Infrapy}
\label{\detokenize{pipeline:running-pipeline-processing-in-infrapy}}\label{\detokenize{pipeline:pipeline}}\label{\detokenize{pipeline::doc}}
The folder tutorials/cli contains all necessary data and configuration files to begin utilizing the pipeline processing methodologies in Infrapy.

Once installed, the steps to run pipeline processing in Infrapy are:
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
Either load local waveform data into a sqlite database or connect to a Oracle database following instructions in {\hyperref[\detokenize{pisces:pisces}]{\sphinxcrossref{\DUrole{std,std-ref}{Interfacing with Pisces}}}}.

\item {} 
Create a configuration file. See {\hyperref[\detokenize{config:config}]{\sphinxcrossref{\DUrole{std,std-ref}{Configuration Files}}}} for a detailed description of the parameters that need to be included. Two example configuration files, one for connecting to the provided sqlite file and one for connecting to an oracle database are provided.

\item {} 
Run the FK analysis for a specific array:

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{infrapy} \PYG{n}{run\PYGZus{}fk} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config\PYGZus{}file} \PYG{n}{BRPConfig}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{3}
\item {} 
Run the FD analysis for a specific array that has already FK results, remember to locate the parameter id from your FK analysis (you can use the script read\_pfk.py to find the correct id).

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  \PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{infrapy} \PYG{n}{run\PYGZus{}fd} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config\PYGZus{}file} \PYG{n}{BRPConfig}\PYG{o}{.}\PYG{n}{txt}

\PYG{l+m+mf}{4.} \PYG{n}{Once} \PYG{n}{you} \PYG{n}{have} \PYG{n}{run} \PYG{n}{detection} \PYG{n}{on} \PYG{l+m+mi}{2}\PYG{o}{+} \PYG{n}{array}\PYG{p}{,} \PYG{n}{run} \PYG{n}{association} \PYG{n}{processing}\PYG{p}{:}

\PYG{o}{.}\PYG{o}{.} \PYG{n}{code}\PYG{o}{\PYGZhy{}}\PYG{n}{block}\PYG{p}{:}\PYG{p}{:} \PYG{n}{python}

    \PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{infrapy} \PYG{n}{run\PYGZus{}assoc} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config\PYGZus{}file} \PYG{n}{BRPConfig}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}


\paragraph{Configuration Files}
\label{\detokenize{config:configuration-files}}\label{\detokenize{config:config}}\label{\detokenize{config::doc}}
We run the different steps of the processing by running a script along with a configuration file. This applies to both station and network analysis levels. Configuration files set the parameters that are required to perform a specific analysis. The configuration file for the array processing (station level analysis) has the following structure:

An example configuration file is provided in tutorials/.  Parameters for each field within the configuration file are outlined below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} configuration file to run array (FK) processing and (FD) detection

[database] \PYGZsh{} required
url = sqlite:///example.sqlite
\PYGZsh{} database for processing
site = pisces.tables.css3:Site
wfdisc = pisces.tables.css3:Wfdisc
affiliation = pisces.tables.css3:Affiliation
\PYGZsh{} schemas for tables utilized in processing
\PYGZsh{} if processing in sqlite database, tables remain the same
\PYGZsh{} if processing in Oracle database, tables should point to your custom global\PYGZus{}.py file

[GeneralParams]
year=2012
\PYGZsh{} year for processing
dayofyearini=100
\PYGZsh{} Julian Day to begin processing
dayofyearend=102
\PYGZsh{} Julian Day to stop processing
station=BRP
\PYGZsh{} REFSTA of station for processing
channel=EDF
\PYGZsh{} channel
name=example
\PYGZsh{} name of processing parameters
cpucnt=30
\PYGZsh{} number of cpu cores to use for processing
domain=time
\PYGZsh{} domain (time or frequency) to run FK and FD processing


[FKParams]
name=mid band fk test
\PYGZsh{} name for fk processing
freqmin=0.5
\PYGZsh{} minimum frequency
freqmax=5.0
\PYGZsh{} maximum frequency
beamwinlen=60
\PYGZsh{} beam window length
beamwinstep=30
\PYGZsh{} beam window step
backazmin=\PYGZhy{}180.0
\PYGZsh{} minimum bz for processing
backazmax=180.0
\PYGZsh{} maximum bz for processing
backazstep=1.5
\PYGZsh{} bz step
trvelmin=300.0
\PYGZsh{} minimum trace velocity
trvelmax=600.0
\PYGZsh{} maximum trace velocity
trvelstep=2.5
\PYGZsh{} trace velocity step
beammethod=bartlett
\PYGZsh{} beam method
fkresults=fk\PYGZus{}res\PYGZus{}brp
\PYGZsh{} where fk processing results are stored
numsources = 1
func\PYGZus{}fk = None

[FDetectParams]
back\PYGZus{}az\PYGZus{}lim=10
\PYGZsh{} limit of bz deviation between consecutive fk results
detwinlen=300.0
\PYGZsh{} window length for adaptive f detection
detthresh=0.99
\PYGZsh{} detection threshold
dsegmin=5
detmethod=fstat
tb\PYGZus{}prod=4000
adaptivewlen=1200
\PYGZsh{}length of window for AFD
pthreshold=.01
\PYGZsh{}p\PYGZhy{}value for time domain detection
pfkid=0
\PYGZsh{} pkfid for FK results
corrthreshold=0.5
\PYGZsh{} threshold for correlation values
mineventlength
\PYGZsh{} minimum event length in seconds
fkresults=fk\PYGZus{}res\PYGZus{}brp
\PYGZsh{} fk results to run detection on
fdresults=fd\PYGZus{}res\PYGZus{}example\PYGZus{}brp
\PYGZsh{} where detection results are saved



[AssocLocParams]
network=YJ
\PYGZsh{} network for association
pfdetectid=0
\PYGZsh{} detection ID from detection processing (all arrays for assoc must have same detect ID)
pfkid=2
\PYGZsh{} beamforming ID (all arrays must have same FK ID)
beamwidth=10.0
rangemax=1000.0
clusterthresh=4.0
trimthresh=None
eventdetmin=3
\PYGZsh{} minimum \PYGZsh{} of detections to form event
eventarrmin=2
\PYGZsh{} minimum number of arrays for event
duration = 60
\PYGZsh{} duration (minutes) for association windows

fdtable\PYGZus{}1=fd\PYGZus{}res\PYGZus{}example\PYGZus{}brp
\PYGZsh{}fdtable\PYGZus{}2=fd\PYGZus{}res\PYGZus{}fsu
\PYGZsh{}fdtable\PYGZus{}3=fd\PYGZus{}res\PYGZus{}wmu
\PYGZsh{} tables where detection results are stored
resultstable = test\PYGZus{}assoc
\PYGZsh{} table where association results will be stored
\end{sphinxVerbatim}


\paragraph{Infrapy FK Processing}
\label{\detokenize{fk:infrapy-fk-processing}}\label{\detokenize{fk:fk}}\label{\detokenize{fk::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{infrapy} \PYG{n}{run\PYGZus{}fk} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config\PYGZus{}file} \PYG{n}{BRPConfig}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}


\paragraph{Infrapy Detection (FD) Processing}
\label{\detokenize{fd:infrapy-detection-fd-processing}}\label{\detokenize{fd:fd}}\label{\detokenize{fd::doc}}
Infrapy detects signals using an Adaptive F-Detector (AFD). The adaptive F-detector was developed by Arrowsmith et al., (2009) to account for both correlated and uncorrelated noise through modification of the conventional F-statistic. The detector accounts for temporal changes in noise by applying an adaptive window to update the detection distribution, which allows for the distinction between signal and correlated noise.

\noindent\sphinxincludegraphics{{AFD}.png}


\subparagraph{Configuration file}
\label{\detokenize{fd:configuration-file}}
Detection is run using the same configuration file as FK processing.  Detection requires input from FK processing results

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{infrapy} \PYG{n}{run\PYGZus{}fd} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config\PYGZus{}file} \PYG{n}{BRPConfig}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}


\paragraph{Infrapy Assoc Processing}
\label{\detokenize{assoc:infrapy-assoc-processing}}\label{\detokenize{assoc:assoc}}\label{\detokenize{assoc::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{infrapy} \PYG{n}{run\PYGZus{}assoc} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{config\PYGZus{}file} \PYG{n}{BRPConfig}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}


\paragraph{Scripts}
\label{\detokenize{scripts:scripts}}\label{\detokenize{scripts:id1}}\label{\detokenize{scripts::doc}}

\subparagraph{Scripts to manipulate Infrapy FK results}
\label{\detokenize{scripts:scripts-to-manipulate-infrapy-fk-results}}
Use “any\_command”.py -h to get specific information to run the script
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
read\_pfk.py to see the different set of configuration parameters used previously for FK analysis

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] -h    \textendash{}help     show this help message and exit
\item[] -d    SQ      name of the database connection, e.g.: -d sqlite:///UT\_tutorial.sqlite
\end{DUlineblock}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{read\PYGZus{}pfk}\PYG{o}{.}\PYG{n}{py} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{n}{d} \PYG{n}{SQ}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{1}
\item {} 
print\_rfk.py to see fk results for a specific array and FK parameter ID

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] -h \textendash{}help     show this help message and exit
\item[] -d SQ         name of the database connection, e.g.: -d sqlite:///UT\_tutorial.sqlite
\item[] -a ARRAY      array name, e.g.: -HWU4
\item[] -t FKRESULTS  specific table with results, e.g.: -t fk\_HWU
\item[] -s TS        starttime plot, e.g.: -s /‘2014-03-02T00:00:00/’
\item[] -e TE        endtime plot, e.g.: -s /‘2014-03-03T00:00:00/’
\item[] -F FVAL      limit Fvalue, e.g.: -F 0
\item[] -o OUTTEXT    print fk data, e.g.: -o res\_FILE
\end{DUlineblock}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{print\PYGZus{}rfk}\PYG{o}{.}\PYG{n}{py} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{n}{d} \PYG{n}{SQ} \PYG{o}{\PYGZhy{}}\PYG{n}{a} \PYG{n}{ARRAY} \PYG{o}{\PYGZhy{}}\PYG{n}{f} \PYG{n}{PFK\PYGZus{}ID} \PYG{o}{\PYGZhy{}}\PYG{n}{t} \PYG{n}{FKRESULTS} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{s} \PYG{n}{TS}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{s} \PYG{n}{TE}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{F} \PYG{n}{FVAL}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{res\PYGZus{}FILE}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{2}
\item {} 
plot1\_rfk.py to plot FK results

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] -h \textendash{}help    show this help message and exit
\item[] -d SQ        name of the database connection, e.g.: -d sqlite:///UT\_tutorial.sqlite
\item[] -a ARRAY     array name, e.g.: -a HWU4
\item[] -f PFK\_ID    FK parameter ID to be plot, e.g.: -f 3
\item[] -t FKRESULTS specific table with results, e.g.: -t fk\_res\_HWU
\item[] -w WAVEPLOT  plot waveforms, e.g.: -w 0
\item[] -s TS        starttime plot, e.g.: -s /‘2014-03-02T00:00:00/’
\item[] -e TE        endtime plot, e.g.: -s /‘2014-03-03T00:00:00/’
\item[] -F FVAL      limit Fvalue, e.g.: -F 0
\item[] -slo SLOFK   limit slofk, e.g.: -slo 0
\item[] -bzmin BZMIN limit min bz, e.g.: -bzmin 0
\item[] -bzmax BZMAX limit max bz, e.g.: -bzmax 360
\end{DUlineblock}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}}\PYG{n}{plot1\PYGZus{}rfk}\PYG{o}{.}\PYG{n}{py} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{n}{d} \PYG{n}{SQ} \PYG{o}{\PYGZhy{}}\PYG{n}{a} \PYG{n}{ARRAY} \PYG{o}{\PYGZhy{}}\PYG{n}{f} \PYG{n}{PFK\PYGZus{}ID} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{t} \PYG{n}{FKRESULTS}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{w} \PYG{n}{WAVEPLOT}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{s} \PYG{n}{TS}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{e} \PYG{n}{TE}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{F} \PYG{n}{FVAL}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{slo} \PYG{n}{SLOFK}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{bzmin} \PYG{n}{BZMIN}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{bzmax} \PYG{n}{BZMAX}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{FKres_Fval}.png}

\noindent\sphinxincludegraphics{{FKres_trcvel}.png}

\noindent\sphinxincludegraphics{{FKres_bz}.png}


\subparagraph{Scripts to manipulate Infrapy FD results}
\label{\detokenize{scripts:scripts-to-manipulate-infrapy-fd-results}}\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
read\_pfd.py to see the different set of configuration parameters used previously for detection analysis

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] -h, \textendash{}help  show this help message and exit
\item[] -d SQ       name of the database connection, e.g.: -d sqlite:///mydb.sqlite
\end{DUlineblock}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{read\PYGZus{}pfd}\PYG{o}{.}\PYG{n}{py} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{n}{d} \PYG{n}{SQ}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{1}
\item {} 
read\_rfd.py to see the available detection results

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] -h, \textendash{}help            show this help message and exit
\item[] -d SQ                 name of the database connection, e.g.: -db sqlite:///UT\_tutorial.sqlite
\item[] -a ARRAY              array name, e.g.: -a HWU4
\item[] -f PFK\_ID, \textendash{}pfkid PFK\_ID FK parameter ID to be plot, e.g.: -f 0
\item[] -j PFDID, \textendash{}pfdid PFDID fd parameter id, e.g.: -j 0
\item[] -t FDRESULTS          specific table with results, e.g.: -t fd\_res\_HWU
\end{DUlineblock}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{read\PYGZus{}rfd}\PYG{o}{.}\PYG{n}{py} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{n}{d} \PYG{n}{SQ} \PYG{o}{\PYGZhy{}}\PYG{n}{a} \PYG{n}{ARRAY} \PYG{o}{\PYGZhy{}}\PYG{n}{f} \PYG{n}{PFK\PYGZus{}ID} \PYG{o}{\PYGZhy{}}\PYG{n}{j} \PYG{n}{PFDID} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{t} \PYG{n}{FDRESULTS}\PYG{p}{]}
\PYG{n}{fdid}\PYG{p}{:} \PYG{l+m+mi}{1}  \PYG{n}{pfdid}\PYG{p}{:} \PYG{l+m+mi}{0} \PYG{n}{pfkid}\PYG{p}{:} \PYG{l+m+mi}{0}   \PYG{n}{timeini}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{00}\PYG{p}{:}\PYG{l+m+mi}{24}\PYG{p}{:}\PYG{l+m+mi}{30}   \PYG{n}{timeend}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{00}\PYG{p}{:}\PYG{l+m+mi}{26}\PYG{p}{:}\PYG{l+m+mo}{00}   \PYG{n}{maxf0}\PYG{p}{:} \PYG{l+m+mf}{5.54282460217} \PYG{l+m+mf}{0.60800443287}
\PYG{n}{fdid}\PYG{p}{:} \PYG{l+m+mi}{2}  \PYG{n}{pfdid}\PYG{p}{:} \PYG{l+m+mi}{0} \PYG{n}{pfkid}\PYG{p}{:} \PYG{l+m+mi}{0}   \PYG{n}{timeini}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{00}\PYG{p}{:}\PYG{l+m+mi}{47}\PYG{p}{:}\PYG{l+m+mo}{00}   \PYG{n}{timeend}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{00}\PYG{p}{:}\PYG{l+m+mi}{53}\PYG{p}{:}\PYG{l+m+mi}{30}   \PYG{n}{maxf0}\PYG{p}{:} \PYG{l+m+mf}{3.67253815208} \PYG{l+m+mf}{0.46716764663}
\PYG{n}{fdid}\PYG{p}{:} \PYG{l+m+mi}{3}  \PYG{n}{pfdid}\PYG{p}{:} \PYG{l+m+mi}{0} \PYG{n}{pfkid}\PYG{p}{:} \PYG{l+m+mi}{0}   \PYG{n}{timeini}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{00}\PYG{p}{:}\PYG{l+m+mi}{54}\PYG{p}{:}\PYG{l+m+mi}{30}   \PYG{n}{timeend}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{00}\PYG{p}{:}\PYG{l+m+mi}{56}\PYG{p}{:}\PYG{l+m+mi}{30}   \PYG{n}{maxf0}\PYG{p}{:} \PYG{l+m+mf}{2.61098286001} \PYG{l+m+mf}{0.372113714177}
\PYG{n}{fdid}\PYG{p}{:} \PYG{l+m+mi}{4}  \PYG{n}{pfdid}\PYG{p}{:} \PYG{l+m+mi}{0} \PYG{n}{pfkid}\PYG{p}{:} \PYG{l+m+mi}{0}   \PYG{n}{timeini}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{01}\PYG{p}{:}\PYG{l+m+mi}{47}\PYG{p}{:}\PYG{l+m+mi}{30}   \PYG{n}{timeend}\PYG{p}{:} \PYG{l+m+mi}{12}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{08}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{14} \PYG{l+m+mo}{01}\PYG{p}{:}\PYG{l+m+mi}{49}\PYG{p}{:}\PYG{l+m+mo}{00}   \PYG{n}{maxf0}\PYG{p}{:} \PYG{l+m+mf}{9.91099311406} \PYG{l+m+mf}{0.749628285504}
\end{sphinxVerbatim}
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\setcounter{enumi}{2}
\item {} 
read\_rfd\_fast.py to write the available detection results in text file

\end{enumerate}

\begin{DUlineblock}{0em}
\item[] -h, \textendash{}help            show this help message and exit
\item[] -d SQ                 name of the database connection, e.g.: -d sqlite:///UT\_tutorial.sqlite
\item[] -a ARRAY              array name, e.g.: -a I37NO
\item[] -f PFKID, \textendash{}pfkid PFKID fk parameter id, e.g.: -f 0
\item[] -j PFDID, \textendash{}pfdid PFDID fd parameter id, e.g.: -j 0
\item[] -t FDRESULTS          specific table with results, e.g.: -t fd\_I37
\item[] -o OUTTEXT            fd parameter id, e.g.: -o res\_FILE
\end{DUlineblock}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{read\PYGZus{}rfd\PYGZus{}fast}\PYG{o}{.}\PYG{n}{py} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{h}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{n}{d} \PYG{n}{SQ} \PYG{o}{\PYGZhy{}}\PYG{n}{a} \PYG{n}{ARRAY} \PYG{o}{\PYGZhy{}}\PYG{n}{f} \PYG{n}{PFKID} \PYG{o}{\PYGZhy{}}\PYG{n}{j} \PYG{n}{PFDID} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{t} \PYG{n}{FDRESULTS}\PYG{p}{]} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{o} \PYG{n}{OUTTEXT}\PYG{p}{]}
\end{sphinxVerbatim}


\section{Tutorial}
\label{\detokenize{tutorial:tutorial}}\label{\detokenize{tutorial:id1}}\label{\detokenize{tutorial::doc}}
A series of jupyter notebooks illustrating how to use infrapy subroutines are found in the /tutorial folder.
You can run these by navigating to that folder and running:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZgt{}\PYGZgt{} jupyter notebook
\end{sphinxVerbatim}

A browser will launch with a webpage showing a directory listing, click on the InfraPyTutorial.ipynb link to open the main window, which will provide links to the various tutorials.


\section{Schema}
\label{\detokenize{schema:schema}}\label{\detokenize{schema:id1}}\label{\detokenize{schema::doc}}
The purpose of this document is to define the schema used for the
operation of the infrasound analysis tool, infrapy. The tables described
by this document extend the CSS3.0 or KB core schema to include
information required for the operation of infrapy. This document is
divided into three sections, the first being this introduction. Section
two defines eight new, infrasonic data processing-specific database
tables. Both internal (ORACLE) and external formats for the attributes
are defined, along with a short description of each attribute. Section
three of the document shows the relationships between the different
tables by using entity-relationship diagrams.

This schema is a work in progress and may be updated as development of
infrapy continues.


\subsection{Table Descriptions}
\label{\detokenize{schema:table-descriptions}}
This section describes the logical structure of each table used in the
Infrapy software package. The name of the table is first, followed by a
description of the purpose and use of the table. Below the description
is a listing of the columns, in the order which they are defined in the
tables. The storage column gives the actual ORACLE datatype for the
column in question. The external format and character positions columns
are provided for the convenience of database users who wish to transfer
data between the ORACLE database tables and flat files.


\subsubsection{Conventions}
\label{\detokenize{schema:conventions}}
The following conventions are used, following Carr et al (2002):


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxstylestrong{Element}
&\sphinxstyletheadfamily 
\sphinxstylestrong{Appearance}
&\sphinxstyletheadfamily 
\sphinxstylestrong{Example}
\\
\hline
Database table
&
Bold
&
\sphinxstylestrong{arrival}
\\
\hline
Database columns
&
Italic
&
\sphinxstyleemphasis{sta}
\\
\hline
Database table and
column when written
in the dot notation
&
Bold.italic
&
\sphinxstylestrong{arrival}\sphinxstyleemphasis{.sta}
\\
\hline
Value of a key or
component of a key
&
Courier font
&
arid
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Table Definitions: Infrapy-Specific Tables}
\label{\detokenize{schema:table-definitions-infrapy-specific-tables}}
Table descriptions can be found in the API section of the documentation.


\section{API}
\label{\detokenize{infrapy:api}}\label{\detokenize{infrapy:id1}}\label{\detokenize{infrapy::doc}}

\subsection{Association}
\label{\detokenize{infrapy.association:association}}\label{\detokenize{infrapy.association::doc}}

\subsubsection{HJL}
\label{\detokenize{infrapy.association:hjl}}

\subsection{Characterization}
\label{\detokenize{infrapy.characterization:characterization}}\label{\detokenize{infrapy.characterization::doc}}

\subsubsection{SPYE}
\label{\detokenize{infrapy.characterization:spye}}

\subsection{Database Processing}
\label{\detokenize{infrapy.database:database-processing}}\label{\detokenize{infrapy.database::doc}}

\subsubsection{Database Processing Taskbase}
\label{\detokenize{infrapy.database.taskbase:database-processing-taskbase}}\label{\detokenize{infrapy.database.taskbase::doc}}

\subsubsection{Database Schema}
\label{\detokenize{infrapy.database:database-schema}}

\subsection{Beamforming/Detection}
\label{\detokenize{infrapy.detection:beamforming-detection}}\label{\detokenize{infrapy.detection::doc}}

\subsubsection{Beamforming}
\label{\detokenize{infrapy.detection:beamforming}}

\subsubsection{Beamforming\_new}
\label{\detokenize{infrapy.detection:beamforming-new}}

\subsection{Location}
\label{\detokenize{infrapy.location:location}}\label{\detokenize{infrapy.location::doc}}

\subsubsection{BISL}
\label{\detokenize{infrapy.location:bisl}}

\subsection{Propagation}
\label{\detokenize{infrapy.propagation:propagation}}\label{\detokenize{infrapy.propagation::doc}}

\subsubsection{Infrasound}
\label{\detokenize{infrapy.propagation:infrasound}}

\subsubsection{Likelihoods}
\label{\detokenize{infrapy.propagation:likelihoods}}

\subsubsection{Seismic}
\label{\detokenize{infrapy.propagation:seismic}}

\subsection{Utils}
\label{\detokenize{infrapy.utils:utils}}\label{\detokenize{infrapy.utils::doc}}

\subsubsection{infrapy.utils.cart2pol module}
\label{\detokenize{infrapy.utils:infrapy-utils-cart2pol-module}}

\subsubsection{infrapy.utils.confidence module}
\label{\detokenize{infrapy.utils:infrapy-utils-confidence-module}}

\subsubsection{infrapy.utils.db2db module}
\label{\detokenize{infrapy.utils:infrapy-utils-db2db-module}}

\subsubsection{infrapy.utils.db2sac module}
\label{\detokenize{infrapy.utils:infrapy-utils-db2sac-module}}

\subsubsection{infrapy.utils.files2db module}
\label{\detokenize{infrapy.utils:infrapy-utils-files2db-module}}

\subsubsection{infrapy.utils.get\_arraywaveforms module}
\label{\detokenize{infrapy.utils:infrapy-utils-get-arraywaveforms-module}}

\subsubsection{infrapy.utils.get\_header\_table module}
\label{\detokenize{infrapy.utils:infrapy-utils-get-header-table-module}}

\subsubsection{infrapy.utils.get\_mean\_locations module}
\label{\detokenize{infrapy.utils:infrapy-utils-get-mean-locations-module}}

\subsubsection{infrapy.utils.latlon module}
\label{\detokenize{infrapy.utils:infrapy-utils-latlon-module}}

\subsubsection{infrapy.utils.ms2db module}
\label{\detokenize{infrapy.utils:infrapy-utils-ms2db-module}}

\subsubsection{infrapy.utils.obspy\_conversion module}
\label{\detokenize{infrapy.utils:infrapy-utils-obspy-conversion-module}}

\subsubsection{infrapy.utils.prog\_bar module}
\label{\detokenize{infrapy.utils:infrapy-utils-prog-bar-module}}

\subsubsection{infrapy.utils.ref2sac module}
\label{\detokenize{infrapy.utils:infrapy-utils-ref2sac-module}}

\subsubsection{infrapy.utils.sac\_stats module}
\label{\detokenize{infrapy.utils:infrapy-utils-sac-stats-module}}

\subsubsection{infrapy.utils.seed2sac module}
\label{\detokenize{infrapy.utils:infrapy-utils-seed2sac-module}}

\subsubsection{infrapy.utils.short\_time module}
\label{\detokenize{infrapy.utils:infrapy-utils-short-time-module}}

\subsubsection{infrapy.utils.skew\_norm module}
\label{\detokenize{infrapy.utils:infrapy-utils-skew-norm-module}}

\subsubsection{infrapy.utils.zip2ref module}
\label{\detokenize{infrapy.utils:infrapy-utils-zip2ref-module}}
Welcome to Infrapy’s documentation.  Get started with {\hyperref[\detokenize{installation:installation}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation}}}} and then get an overview with the {\hyperref[\detokenize{quickstart:quickstart}]{\sphinxcrossref{\DUrole{std,std-ref}{Quickstart}}}}.  There is also a more detailed {\hyperref[\detokenize{tutorial:tutorial}]{\sphinxcrossref{\DUrole{std,std-ref}{Tutorial}}}} that demonstrates the various processing capabilities of infrapy.  The rest of the docs describe each component in detail, with a full reference in the {\hyperref[\detokenize{infrapy:api}]{\sphinxcrossref{\DUrole{std,std-ref}{API}}}} section.

This document is a work in progress and may be updated as development of Infrapy continues.

\noindent\sphinxincludegraphics{{infrapy}.png}


\chapter{User’s Guide}
\label{\detokenize{index:user-s-guide}}
This part of the documentation, which is mostly prose, begins with some background information about Infrapy, then focuses on step-by-step instructions for data processing using Infrapy.


\section{Overview}
\label{\detokenize{index:overview}}
Infrapy is a tool for processing infrasound and seismic array data. Infrapy implements a database-centric
approach for pipeline continuous near real-time analysis. The pipeline includes analysis at station and network levels (using beam-forming and clustering techniques, respectively) for the
detection, association and location of events.  The pipeline relies on the interaction of the algorithms with a relational database structure to organize and store waveform data, the
parameters for the analysis, and results of both levels of analysis. Our implementation can interact seamlessly with traditional (e.g.: Oracle) and serverless (e.g.: SQLite) relational databases.



\renewcommand{\indexname}{Index}
\printindex
\end{document}